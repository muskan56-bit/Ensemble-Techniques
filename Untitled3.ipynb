{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Ensemble Learning"
      ],
      "metadata": {
        "id": "tkFnazoV53jH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n"
      ],
      "metadata": {
        "id": "PScSun0S5-jf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-ensemble Learning is a general machine learning paradigm where the goal is to improve predictive performance—either classification accuracy or regression stability—by combining the predictions of multiple individual models instead of relying on a single model.The term \"ensemble\" is borrowed from music and theater, where it refers to a group of performers working together to produce a better result than any individual performer could achieve alone.\n",
        "key idea ensemble learning-\n",
        "1. Diversity: Combining Different Perspectives\n",
        "2. Error Reduction (Bias vs. Variance)\n"
      ],
      "metadata": {
        "id": "KSPr2fwnFOpF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.2: What is the difference between Bagging and Boosting?"
      ],
      "metadata": {
        "id": "kbxH3Cz86G3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-bagging -\n",
        "1)Parallel and Independent. Each model is trained in parallel, completely independent of the others.\n",
        "2)All individual models are treated equally (same voting power in the final result).\n",
        "3)Each model is trained on a random sample with replacement (a bootstrap sample) of the original training data.\n",
        "4)Reduce Variance. Aims to stabilize a high-variance model (like a deep decision tree) by averaging out the noise introduced by data variations.\n",
        "\n",
        "boosting-\n",
        "1) Reduce Variance. Aims to stabilize a high-variance model (like a deep decision tree) by averaging out the noise introduced by data variations.\n",
        "2)Models are weighted; better-performing models (those with lower error) have greater influence.\n",
        "3)Each model is trained on the full dataset, but the data points (or targets/residuals) are adjusted to highlight previous errors.\n",
        "4)Reduce Bias. Aims to reduce the systemic errors left by simpler models by forcing successive models to correct them.\n",
        "\n"
      ],
      "metadata": {
        "id": "2tYn4jeyFokd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.3: What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?"
      ],
      "metadata": {
        "id": "q5OISV4Z6OX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- bootstrap sampling (or simply bootstrapping) is a statistical resampling technique used to estimate the characteristics of a population by sampling from a single data sample.\n",
        "\n",
        "role-\n",
        "1. Introducing Diversity (Reducing Variance)\n",
        "\n",
        "2.De-correlating the Trees (The Random Forest Advantage)\n"
      ],
      "metadata": {
        "id": "ONnwp5n4GcMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.4: What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n"
      ],
      "metadata": {
        "id": "LG8xg-p26TLG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Out-of-Bag (OOB) samples are the data points from the original training set that were not included in the bootstrap sample used to train a specific base estimator (tree).\n",
        "\n",
        "how OOB score used to evaluate models-\n",
        "To calculate the OOB prediction for a single data point, x:Identify all the trees in the forest for which $x_i$ was an OOB sample (i.e., trees that never saw $x_i$ during training).Have those trees make a prediction for $x_i$.Aggregate these predictions (e.g., take the majority vote for classification or the average for regression).\n",
        "\n",
        "OOB score is then calculated by comparing these OOB predictions against the true target values for every single data point in the entire original training set.\n"
      ],
      "metadata": {
        "id": "DuTdlj37G0ya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.5: Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n"
      ],
      "metadata": {
        "id": "PBur1UKj6Ydg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- decision tree -\n",
        "1)Reduction in Impurity (Gini or Entropy) at each split.\n",
        "2)Low. Highly unstable and prone to bias.\n",
        "3)Biased. Heavily favors a single feature in a group of highly correlated features, ignoring the others.\n",
        "4)High. Simple to trace why the single most important feature was chosen (it's the root node).\n",
        "5)Useful for initial quick insights, especially for visualization and understanding the raw logic.\n",
        "\n",
        "random forest -\n",
        "1)Average Reduction in Impurity across all trees in the forest.\n",
        "2)High. Very stable and robust due to averaging.\n",
        "3)Fairer, but still a challenge. It is less biased because trees are trained on different data subsets, allowing correlated features to take turns being selected.\n",
        "4)Low to Moderate. The final score is an average across hundreds of trees, making it impossible to trace an individual decision path.\n",
        "5)Standard and Reliable. Used in production to select the final feature set, guide data collection, and provide model explainability.\n",
        "\n"
      ],
      "metadata": {
        "id": "n47_KE6vHdPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.n 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores"
      ],
      "metadata": {
        "id": "LWLFOQas6cKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "print(\"Loading Breast Cancer dataset...\")\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(\"\\nTraining Random Forest Classifier...\")\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=500,\n",
        "    max_depth=10,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training complete.\")\n",
        "\n",
        "\n",
        "feature_scores = rf_model.feature_importances_\n",
        "\n",
        "feature_names = X.columns\n",
        "importance_df = pd.Series(feature_scores, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "top_n = 5\n",
        "top_features = importance_df.head(top_n)\n",
        "\n",
        "print(f\"Top {top_n} Features by Gini Importance (Random Forest)\")\n",
        "\n",
        "for rank, (feature, score) in enumerate(top_features.items(), 1):\n",
        "\n",
        "    print(f\"[{rank}] {feature:<25} : {score:.4f}\")\n",
        "\n",
        "print(f\"Total features considered: {len(feature_names)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "absXz3A-IXQv",
        "outputId": "21ed56c8-ee44-4d04-aa1d-beafd42bcd7c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Breast Cancer dataset...\n",
            "\n",
            "Training Random Forest Classifier...\n",
            "Training complete.\n",
            "Top 5 Features by Gini Importance (Random Forest)\n",
            "[1] worst area                : 0.1300\n",
            "[2] worst perimeter           : 0.1280\n",
            "[3] worst concave points      : 0.1272\n",
            "[4] worst radius              : 0.0928\n",
            "[5] mean concave points       : 0.0890\n",
            "Total features considered: 30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "gL6mYqJ56gc2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "print(\"Loading Iris dataset...\")\n",
        "\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.3,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "print(f\"Testing set size: {X_test.shape[0]} samples\")\n",
        "print(\"\\n[A] Training Single Decision Tree...\")\n",
        "dt_model = DecisionTreeClassifier(\n",
        "    max_depth=None,\n",
        "    random_state=4\n",
        ")\n",
        "\n",
        "dt_model.fit(X_train, y_train)\n",
        "dt_pred = dt_model.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "\n",
        "print(\"[B] Training Bagging Classifier (Ensemble)...\")\n",
        "\n",
        "\n",
        "bagging_model = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(max_depth=None, random_state=42),\n",
        "    n_estimators=100,\n",
        "    max_samples=1.0,\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "bagging_accuracy = accuracy_score(y_test, bagging_pred)\n",
        "\n",
        "\n",
        "\n",
        "print(\"     MODEL ACCURACY COMPARISON (IRIS DATASET)\")\n",
        "print(f\"1. Single Decision Tree Accuracy : {dt_accuracy:.4f}\")\n",
        "print(f\"2. Bagging Classifier Accuracy : {bagging_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "if bagging_accuracy > dt_accuracy:\n",
        "    print(\"Conclusion: Bagging successfully improved the model's accuracy, reducing variance.\")\n",
        "elif bagging_accuracy < dt_accuracy:\n",
        "    print(\"Conclusion: The single Decision Tree was slightly more accurate on this specific test set.\")\n",
        "else:\n",
        "    print(\"Conclusion: Both models achieved the same accuracy.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM6Km6fdJH7P",
        "outputId": "5d8c7601-ecd6-4c68-86ac-ad6f353b6dbe"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Iris dataset...\n",
            "Training set size: 105 samples\n",
            "Testing set size: 45 samples\n",
            "\n",
            "[A] Training Single Decision Tree...\n",
            "[B] Training Bagging Classifier (Ensemble)...\n",
            "     MODEL ACCURACY COMPARISON (IRIS DATASET)\n",
            "1. Single Decision Tree Accuracy : 0.9778\n",
            "2. Bagging Classifier Accuracy : 0.9333\n",
            "Conclusion: The single Decision Tree was slightly more accurate on this specific test set.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n"
      ],
      "metadata": {
        "id": "3FnCrMaH6krQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# --- 1. Load the Dataset ---\n",
        "print(\"Loading Breast Cancer dataset...\")\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "\n",
        "\n",
        "print(\"\\nSetting up GridSearchCV...\")\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "}\n",
        "\n",
        "\n",
        "rf_base = RandomForestClassifier(random_state=42)\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf_base,\n",
        "    param_grid=param_grid,\n",
        "    scoring='accuracy',\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Starting Grid Search...\")\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Grid Search complete.\")\n",
        "\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "best_rf_model = grid_search.best_estimator_\n",
        "\n",
        "\n",
        "y_pred = best_rf_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "print(\"  Random Forest Tuning Results (GridSearchCV)\")\n",
        "print(f\"Best Parameters Found: {best_params}\")\n",
        "print(f\"Best Cross-Validation Score: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Final Test Set Accuracy: {final_accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "wBwyR_l1JxJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n"
      ],
      "metadata": {
        "id": "VavJLSWX6oJI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "\n",
        "print(\"Loading California Housing dataset...\")\n",
        "data = fetch_california_housing()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
        "\n",
        "print(\"\\n[A] Training Bagging Regressor...\")\n",
        "\n",
        "\n",
        "base_estimator = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "bagging_model = BaggingRegressor(\n",
        "    estimator=base_estimator,\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "bagging_model.fit(X_train, y_train)\n",
        "bagging_pred = bagging_model.predict(X_test)\n",
        "bagging_mse = mean_squared_error(y_test, bagging_pred)\n",
        "print(\"[B] Training Random Forest Regressor...\")\n",
        "\n",
        "rf_model = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "rf_pred = rf_model.predict(X_test)\n",
        "rf_mse = mean_squared_error(y_test, rf_pred)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"       REGRESSOR PERFORMANCE COMPARISON (CALIFORNIA HOUSING)\")\n",
        "print(\"=\"*70)\n",
        "print(f\"1. Bagging Regressor MSE      : {bagging_mse:.4f}\")\n",
        "print(f\"2. Random Forest Regressor MSE: {rf_mse:.4f}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "if rf_mse < bagging_mse:\n",
        "    print(\"Conclusion: Random Forest achieved a lower MSE, demonstrating the benefit of feature randomness.\")\n",
        "elif rf_mse > bagging_mse:\n",
        "    print(\"Conclusion: Bagging Regressor performed better on this test set.\")\n",
        "else:\n",
        "    print(\"Conclusion: Both models performed equally well.\")"
      ],
      "metadata": {
        "id": "9jzGEzpVJ45F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "q.no.10: You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n"
      ],
      "metadata": {
        "id": "R9muIg-s6tXF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "explaination -\n",
        ". Choice Between Bagging or Boosting\n",
        "Decision: Boosting (specifically, a highly optimized algorithm like XGBoost or CatBoost) is the preferred choice.\n",
        "\n",
        "prove:\n",
        "\n",
        "Goal: Reducing Bias for High Accuracy: Loan default prediction is a high-bias problem. Simple models struggle to identify the complex, non-linear relationships that cause someone to default. Boosting's sequential, error-correcting nature is specifically designed to reduce bias and achieve superior, state-of-the-art accuracy, which is paramount when millions of dollars are at stake.\n",
        "\n",
        "Handling Imbalance: While both can handle imbalance, boosting (when combined with techniques like SMOTE or focused cost-sensitive learning) is better at adjusting its focus to correctly predict the rare \"default\" events.\n",
        "\n",
        "\n",
        ". Base Model Selection\n",
        "Choice: Decision Trees are the ideal base model for both Bagging and Boosting.\n",
        "prove:\n",
        "\n",
        " Boosting (Preferred): Use shallow decision trees (often called \"stumps\" or trees with a max_depth between 3 and 7). Shallow trees are weak, high-bias learners. Boosting combines many of these weak learners, allowing the ensemble to build a complex, low-bias model without over-relying on the rules of any single tree.\n",
        "\n",
        "\n",
        " Handling Overfitting (Regularization)Overfitting is a major risk in boosting, as it continuously fits the residuals, which can eventually lead it to model noise.Learning Rate (Shrinkage): The most effective technique. When adding a new tree's prediction to the ensemble, we scale it by a small learning rate . This forces the model to learn slowly, making the training process more robust and preventing the model from becoming too specialized to the training data.Subsampling: Train each new tree on a random fraction of the training data (e.g., 70% of rows). This introduces randomness (similar to Bagging) and prevents trees from becoming overly correlated, stabilizing the model.Column Subsampling (Feature Randomness): Train each tree using only a random subset of the features. This is especially useful for high-dimensional financial data to increase diversity and prevent reliance on any single feature.Max Depth Constraint: Limiting the max_depth of the base trees, as mentioned above, acts as a primary form of regularization, preventing any single tree from becoming too complex.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JW6x_WPKJ_QO"
      }
    }
  ]
}